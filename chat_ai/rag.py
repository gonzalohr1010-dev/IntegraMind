"""rag.py
Simple retrieval-augmented-generation flow: retrieve context and call generator.
Generator is a configurable function (default: API stub using HF/OPENAI env vars).
"""
from __future__ import annotations
import os
import logging
from typing import List, Dict, Any, Optional

from .embeddings import embed_texts, get_model
from .wikipedia import fetch_wikipedia_answer
import os
from .index import FaissIndex, init_meta_db, META_DB


def default_generator(prompt: str) -> str:
    """Generator that prefers cloud LLMs via the project's LLMClient.

    If no cloud key is available, falls back to a lightweight extractive heuristic
    that picks relevant sentences from the prompt's contexts. This improves
    answers when no external LLM is configured.
    """
    try:
        # Use project LLM client if available (respects PREFERRED_LLM env var)
        from .brain import LLMClient
        client = LLMClient()
        # Use the prompt as the input; LLMClient handles OpenAI/HF selection
        out = client.chat(prompt=prompt, system="Eres un asistente conciso y preciso en español.")
        if out:
            return out
    except Exception:
        logging.debug("LLMClient unavailable or failed; falling back to extractive heuristic")

    # Extract contexts and question from the prompt generated by build_prompt
    try:
        # Expect prompt format produced by build_prompt: 'Contexto:\n{ctx}\n\nPregunta: {q}\nRespuesta:'
        parts = prompt.split('\n')
        ctx_lines = []
        q_text = ''
        mode = None
        for line in parts:
            l = line.strip()
            if l.startswith('Contexto:'):
                mode = 'ctx'
                continue
            if l.startswith('Pregunta:'):
                mode = 'q'
                q_text = l.replace('Pregunta:', '').strip()
                continue
            if mode == 'ctx':
                ctx_lines.append(line)
        contexts = '\n'.join(ctx_lines).strip()
        # Simple extractive heuristic: pick sentences from contexts that overlap question tokens
        import re
        def tokenize(s):
            return set(re.findall(r"[a-zA-ZáéíóúñÁÉÍÓÚÑ0-9]+", (s or '').lower()))
        q_terms = tokenize(q_text or prompt)
        if not q_terms:
            return "No encontré esa información."
        # Split contexts into sentences and score
        sentences = re.split(r"(?<=[\.\!\?])\s+", contexts)
        scored = []
        for s in sentences:
            ts = tokenize(s)
            if not ts: continue
            overlap = len(ts & q_terms)
            if overlap == 0: continue
            score = overlap / (len(ts) ** 0.5 * (len(q_terms) ** 0.5))
            scored.append((score, s.strip()))
        if not scored:
            return "No encontré esa información."
        scored.sort(key=lambda x: x[0], reverse=True)
        selected = [s for _, s in scored[:3]]
        return ' '.join(selected)
    except Exception:
        logging.exception('Fallback generator failed')
        return "No encontré esa información."


def build_prompt(question: str, contexts: List[str]) -> str:
    ctx_text = "\n---\n".join(contexts)
    prompt = f"Usa la siguiente información para responder la pregunta de forma concisa.\nContexto:\n{ctx_text}\n\nPregunta: {question}\nRespuesta:"
    return prompt


def answer_question(question: str, retriever: FaissIndex, top_k: int = 3, generator=default_generator):
    model = get_model()
    if model is None:
        raise RuntimeError('Embeddings model no disponible')
    # ``encode_texts`` returns a numpy array; we need a 2‑D array for FAISS
    qv = model.encode_texts([question])
    D, I = retriever.search(qv, k=top_k)
    # read metadata
    import sqlite3
    conn = sqlite3.connect(META_DB)
    c = conn.cursor()
    contexts = []
    for idx in I[0]:
        if idx < 0:
            continue
        c.execute('SELECT text FROM docs WHERE id=?', (int(idx),))
        row = c.fetchone()
        if row:
            contexts.append(row[0])
    conn.close()
    # If no contexts found locally, try Wikipedia as a lightweight fallback
    wiki_used = False
    wiki_meta = None
    if not contexts:
        try:
            wiki = fetch_wikipedia_answer(question, lang=os.getenv('WIKI_LANG', 'es'))
            if wiki and wiki.get('extract'):
                wiki_used = True
                wiki_meta = wiki
                contexts = [wiki.get('extract', '')]
                # Optionally persist the wiki extract into metadata for future retrieval
                try:
                    init_meta_db()
                    conn = sqlite3.connect(META_DB)
                    c = conn.cursor()
                    # use a deterministic negative id for wiki to avoid collisions
                    wiki_id = -abs(hash(wiki.get('title', '')))
                    c.execute('INSERT OR REPLACE INTO docs (id, source, text) VALUES (?, ?, ?)', (wiki_id, f"wikipedia:{wiki.get('title','')}", wiki.get('extract','')))
                    conn.commit()
                    conn.close()
                except Exception:
                    pass
        except Exception:
            pass

    prompt = build_prompt(question, contexts)
    # Attach a small note if we used Wikipedia
    if wiki_used and wiki_meta is not None:
        prompt = f"(Fuente: Wikipedia - {wiki_meta.get('title')}: {wiki_meta.get('url')})\n" + prompt
    return generator(prompt)


def retrieve_contexts(question: str, retriever: FaissIndex, top_k: int = 3):
    """Return list of contexts (id, text, score) for a question using embeddings and the retriever."""
    model = get_model()
    if model is None:
        raise RuntimeError('Embeddings model no disponible')
    qv = model.encode_texts([question])
    D, I = retriever.search(qv, k=top_k)
    import sqlite3
    conn = sqlite3.connect(META_DB)
    c = conn.cursor()
    results = []
    # D: distances, I: ids
    for dist, iid in zip(D[0].tolist(), I[0].tolist()):
        if iid < 0:
            continue
        c.execute('SELECT text, source FROM docs WHERE id=?', (int(iid),))
        row = c.fetchone()
        if row:
            results.append({'id': int(iid), 'source': row[1], 'text': row[0], 'score': float(dist)})
    conn.close()
    return results


def stream_answer(question: str, retriever: FaissIndex, top_k: int = 3, generator=default_generator, chunk_size: int = 128):
    """Stream the generator output in chunks. If generator can stream, prefer that.
    Here we fall back to slicing the final string into chunks to simulate streaming."""
    full = answer_question(question, retriever, top_k=top_k, generator=generator)
    # yield chunk by chunk
    for i in range(0, len(full), chunk_size):
        yield full[i:i+chunk_size]
